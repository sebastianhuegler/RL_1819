%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{textcomp}
\usepackage{subcaption}
\title{\LARGE \bf
On Model-Based, Model-Free Reinforcement Learning and Optimal Control
}


\author{Julia Str\"obel, Sebastian H\"ugler, and Jan Br\"udigam}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}
\setlength{\belowcaptionskip}{-5pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

The control of dynamical systems can be achieved by a variety of approaches with different advantages and drawbacks. In this paper, implementations of model-based reinforcement learning (RL), model-free RL, and optimal control for linear and non-linear dynamical systems are compared and discussed.

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}

The increase of computational power in recent years has allowed for the successful implementation of learning algorithms to control a wide range of dynamical systems. Nonetheless, classical approaches to control problems also provide useful solutions for this class of systems. In the following model-based RL, model-free RL, and optimal control algorithms are presented, evaluated and discussed.
% Therefore, the aim of this paper is to present and discuss advantages and drawbacks of the algorithms model-based reinforcement learning (RL), model-free RL, and optimal control. These methods are applied to a spring-mass system, a pendulum, and a cart-pole.
\section{SYSTEMS}
This section shows the three models that will be controlled by the different algorithms:
\subsection{Spring-Mass-Damper}
The spring-mass system can be described by a system of differential equations
\begin{equation}
\ddot{x}+\frac{D}{m}\dot{x}+\frac{c}{m}x=\frac{F}{m}
\end{equation}
%\begin{equation}\label{eqn:JanSMsys}
%	\dot{x}=
%	\underbrace{
%		\left( {\begin{array}{cc}
%			0 &1\\
%			-\frac{k}{m} &0\end{array} } \right)}_{A}x + 
%	\underbrace{
%		\left( {\begin{array}{cc}
%		0\\
%		\frac{1}{m}\end{array} } \right)}_{B}u,
%\end{equation}
with the parameters $k=1$\,N/m (spring), $c=0$\, Ns/m (friction) and $m=1$\,kg (mass).
\subsection{Pendulum}
The dynamics of the pendulum are described in (\ref{eqn:JanPsys}).
\begin{equation}\label{eqn:JanPsys}
\dot{x}=
	\left( {\begin{array}{cc}
		x_2\\
		\tfrac{u \: - \: \,b\,x_2 \: - \: m\,g\,s\sin(x_1)}{m\,s^2}\end{array} } \right),
\end{equation}
with the input $u$, the parameters $b=0.2$\,sNm/rad (friction), $m=1$\,kg (mass), $g=9.82$\,m/s$^2$ (gravity), and $s=1$\,m (length of the pendulum).
\subsection{Cart Pole}
The cart-pole dynamics are as follows:
\begin{equation}\label{eqn:JanCPsys}
\dot{x}=
\left( {\begin{array}{cc}
	x_2\\
	\frac{2\,m\,l\,x_4^2\,\text{s}_3 \: + \: 3\,m\,g\,\text{s}_3\,\text{c}_3 \: + \: 4(u \: - \: c\,x_2)}{4(M \: + \: m) \: - \: 3\,m\,\text{c}_3^2}\\
	x_4\\
	\frac{-3\,m\,l\,x_4^2\,\text{s}_3\,\text{c}_3 \: - \: 6(M \: + \: m)\,g\,\text{s}_3 \: - \: 6(u \: - \: c\,x_2)\,\text{c}_3}{l(4(M \: + \: m) \: - \: 3\,m\,\text{c}_3^2)}\end{array} } \right),
\end{equation}
with the parameters $m=0.2$\,kg (mass of pendulum),  $M=1$\,kg (mass of cart), $l=0.5$\,m (length of the pole), $g=9.82$\,m/s$^2$ (gravity), $c=0.2$\,sN/m (friction), and where s$_3$ and c$_3$ stand for sin$(x_3)$ and cos$(x_3)$, respectively.

%\subsection{Simulations}
%For the comparison of the following algorithms 
%% the simulation horizon for all models is 2.5 s, 
%the maximum executable force for the spring-mass and cart pole is 10 N, and the maximum torque for the pendulum is 10 Nm. The initial state of spring-mass (pendulum) is 0 m (0\textdegree) and 0 $\frac{m}{s}$ (0$\frac{rad}{s}$). The cart pole starts at the initial angles 10\textdegree, 90\textdegree, or 180\textdegree, and zero position of the cart. 
%The goals are $\underline{x}=(1.5\quad 0)^T$ for the spring-mass and stable upright positions for the pendulum and cart pole.
%%(For comparison the control energy is obtained by cumulative squared control input and the cost of the trajectory is the cumulative square of state variables.)

\section{MODEL-BASED RL}
In model based RL a model of the system to be controlled is learned using measurement data and a control policy is optimized using this model.

% Secondly a policy is optimized using the learned model in order to control the real world system afterwards.

%In a model based reinforcement approach a model is trained at first using data sampled of a real world system. The data is generated by applying a policy on the system. %At first a random and afterwards optimized policies are used. 
%Having trained the artificial model, closed loop simulations using this model are done in order to optimize the policy moving the system to its target state.%, which is necessary to move the model to a predefined target state.
%%+Maybe a few sentences about the general idea/concept of model-based RL+
\subsection{Algorithm}
In this work the PILCO %(Probabilistic Inference for Learning COntrol)
approach %for model based reinforcement learning presented in
from \cite{PILCO_paper} is investigated. % and a rough overview over the algorithm is given. 
In PILCO a probabilistic dynamic model for long term policy planning is trained. The unknown system dynamics are assumed with 
\begin{equation}\label{PILCO_sysEq}
x_t = f\left(x_{t-1},u_{t-1}\right)
\end{equation}
where $x\in \mathbb{R}^D$ are the
% real valued 
system states, $u \in \mathbb{R}^U$ are the control inputs. The aim of PILCO is to find a policy $\pi$ for the unkown system, i.e. the optimal parameters $\theta$ of a controller.% to minimize a cumulative cost $J^\pi\left(\theta\right)$. 

\begin{figure*}[!thp] 
	\centering
	\begin{subfigure}{0.325\textwidth}
	\includegraphics[width=\textwidth]{sdm_quad.png}
	\caption{Spring Damper Mass (trial \#1)\newline}\label{fig:PILCO_sdm}
	\end{subfigure}
	\begin{subfigure}{0.325\textwidth}
	\includegraphics[width=\textwidth]{pend_quad.png}
	\caption{Pendulum (trial \#1 - top, trial \#2 - bottom)\newline}\label{fig:PILCO_pendulum}
	\end{subfigure}
	\begin{subfigure}{0.325\textwidth}
	\includegraphics[width=\textwidth]{cp_quad.png}
	\caption{Cart pole, initial angle: 10\textdegree, angle (orange), position (blue), policy (yellow)}\label{fig:PILCO_cp}
	\end{subfigure}\caption{Figures (a)-(b) show GP training trajectories (red), additional rollouts (yellow), predicted system trajectories (95 \% confidence, blue) and target states (black).}\label{fig:PILCO}
\end{figure*} 
\paragraph{Learning Model Dynamics}
The learning of the model dynamics described in equation (\ref{PILCO_sysEq}) is done using a gaussian process (GP), where $\left(x_{t-1},u_{t-1}\right) \in \mathbb{R}^{D+F}$ serve as inputs and $\Delta_t = x_{t} - x_{t-1} + \epsilon \in \mathbb{R}^D$ with $\epsilon \sim \mathcal{N}\left(0,\Sigma_\epsilon\right)$ serve as outputs. For the GP the squared exponential kernel (SQE) and a prior mean $m\equiv 0$ is chosen. The parameters of the SQE %(length scales, signal/ noise variance) 
are trained using evidence maximization. Finally the one step predictions based on the GP are:
\begin{align}
p(x_t\mid x_{t-1}, u_{t-1}) = \mathcal{N} \left(x_t \mid \mu_t, \mathbf{\Sigma}_t \right),\label{PILCO_GPp}\\
\mu_t = x_{t-1} + \mathbb{E}_f \left[ \Delta_t \right],\, \mathbf{\Sigma}_t = \text{var}_f \left[ \Delta_t \right]\label{PILCO_GPmu_sigma}
\end{align}
Here the mean $\mathbb{E}_f\left[\Delta'\right]$ and the variance $\text{var}_f\left[\Delta'\right]$ of a test input $\Delta'$ can be calculated using the GP's hyperparameters\cite{PILCO_paper}.

\paragraph{Policy Evaluation}
In order to predict a successor state $x_t$ based on the policy input $u_{t-1} = \pi\left(x_{t-1},\theta\right)$ a joint distribution $p\left(\tilde{x}_{t-1}\right) = p\left(x_{t-1},u_{t-1}\right)$ has to be determined. This is done by first integrating out the state $x_{t-1}$ where $\mu_u$ and $\mathbf{\Sigma}_u$ are obtained. Secondly by additionally calculating the cross-covariance $\text{cov}_f = \left[{x}_{t-1},u_{t-1}\right]$ the joint state-policy distribution can be approximated by a Gaussian. This result is then used for predicting $p\left(\Delta_t\right)$ with
\begin{equation}\label{PILCO_eq_pDelta}
p\left(\Delta_t\right) = \int p\left(f(\tilde{x}_{t-1})\mid\tilde{x}_{t-1}\right)\, p\left(\tilde{x}_{t-1}\right) \text{d}\tilde{x}_{t-1}
\end{equation}
Often there is no analytically feasible solution for equation (\ref{PILCO_eq_pDelta}). Therefore the Gaussian of $p\left(\Delta_t\right)$ is approximated using moment matching or linearization \cite{GP_deisenroth}. Wtih the Gaussian of $p\left(\Delta_t\right)$ an approximate Gaussian is obtained for $p\left(x_t\right)$ by
\begin{align}
&\mu_t = \mu_{t-1}+\mu_\Delta, \\
&\mathbf{\Sigma}_t = \mathbf{\Sigma}_{t-1} +\text{cov}\left[x_{t-1},\Delta_{t}\right]+\text{cov}\left[\Delta_t,x_{t-1}\right].
\end{align}
%,\\
%&\text{cov}\left[x_{t-1},\Delta_{t}\right] = \text{cov}\left[x_{t-1},u_{t-1}\right]\mathbf{\Sigma}_u^{-1}\text{cov}\left[u_{t-1},\Delta_{t}\right]
Finally the the expected cost is calculated according to 
\begin{align}
\mathbb{E}\left[c\left(x_t\right)\right] = \int c\left(x_t\right) \mathcal{N}\left(x_t\mid\mu_t,\mathbf{\Sigma}_t\right) \text{d}x_t.
\end{align}
%where a squared exponential function is preferable, since the gradient is analytically computeable. 
The cost $c(x)$ considering the target state $x_{target}$ is a inverted squared exponential function for each state,
%\begin{equation}
%c\left(x\right) = 1-e^{-\frac{\Vert x-x_{target}\Vert^2}{\sigma_c^2}}
%\end{equation}
where the width is determined by $\sigma_c$.
% determines the width of $c\left(x\right)$.
Using this cost function the resulting partial derivatives over $\mu_t$ and $\Sigma_t$ for the gradients in the policy optimization are calculable in a closed form \cite{GP_deisenroth}.
%Using this cost function the partial derivatives over $\mu_t$ and $\Sigma_t$ for the policy optimization using the gradient 
%\begin{equation}
%\frac{\text{d}\mathbb{E}[c(x_t)]}{\text{d}\theta} = \frac{\partial\mathbb{E}[c(x_t)]}{\partial\mu_t}\frac{\text{d}\mu_t}{\text{d}\theta}+\frac{\partial\mathbb{E}[c(x_t)]}{\partial\Sigma_t}\frac{\text{d}\Sigma_t}{\text{d}\theta}
%\end{equation}
%are calculable in a closed form in (see. \cite{GP_deisenroth}).

%The .
%+Algorithm+
%Implementation is based on the PILCO toolbox which blablabla bla bla

\subsection{Results}
The PILCO framework from \cite{PILCO_web} is applied to the three introduced systems, %. %The simulations have been done using the online available PILCO framework for MATLAB , 
using a radial basis function (RBF) controller for the policy. 
Figure \ref{fig:PILCO_sdm} shows that the spring damper mass system is controlled on trail \#1 after optimizing the GP with the data of random exploration in the beginning. This is due to the fact that the linear system has a low complexity. 
%The pseudo-control energy used to bring the system to its steady state is 27.1 and the cost of the trajectory is 3.55.
The pendulum in figure \ref{fig:PILCO_pendulum} is controlled after trial \#2. At trail \#1 the state space of the system is only explored in the angular region from $-\frac{\pi}{2}$ to $\frac{\pi}{2}$. The result is that the GP is only trained with data out of this region and the uncertainty of the GP model grows when the planned trajectory leaves this region (s. blue errorbars in fig. \ref{fig:PILCO_pendulum}). Afer trail \#2 however, the system is stabilized at $-\pi$ for all following trials.
% The trajectories for different inital positions (varying from -0.06 rad to 0.06 rad) are stable as well. 
This shows that PILCO can handle unkown nonlinear systems effectively. This becomes even more obvious when considering results of the cart-pole trajectories. It takes 7 trials for the swing up from 10\textdegree \, inital pole angle, 3 trails from 90\textdegree \, and 2 trails from 180\textdegree . After the swing up the steady state is controlled stable with slight inaccuarcies. Figure \ref{fig:PILCO_cp} shows the swing up from 10\textdegree. Remarkable is capablility to handle the complexity of the trajectory for this swing up with only a few trials.
%is that  the controller uses gravity to let the pole first swing in the other direction and swing it to its target position afterwards. This is due to  the restriction of the controller to 10 N maximum force. 
A linear controller would not be able to plan such a trajectory.  %The control energy used for this is XXXX and the cost of the trajectory is YYYY. % It has to be mentioned that accuracy of the steady state highly depends on the GP model's uncertainty, since slight deviations in the policy applied on the model lead to differing system behaviour.


%Pendulum:
%The pendulum is controlled in its target position after only XY trials. Remarkable is the swing up of the pendulum. Afer the first trails the target state is reached, but the controller is instable at that point, since this region of the state space is not predicted correctly by the GP model. But after this region is trained to the GP the controller is able to swing up the pendulum in one step. 
%
%stability in target position
%swing up 
%number of trials
%control energy
%
%
%Cart-pole:
%
%Since the maximum torque is restriced to 10 Nm the pendulum is first swang to the right and then to the left in order to reach its target position. The control energy need for this swing up is XY $\text{Nm}^2$.
%higher dimensional state space needs to be explored
%number of trials/interactions with model
%control energy used
%policy swing up 
%
%Conclusion:
%Computationally expensive , Matrix inversion
%non convex optimization does not guarantee optimal trajectory
%beneficial for unknown systems / no previous knowledge needed

% being applied Spring Damper Mass
%The spring damper mass system is controlled on first trail after sampling data with random exploration noise. Accuarcy is depending on the random exploration noise and sample time (tradeoff). control energy used is determined according to usquare one can see how the model is optimized with the iteration steps
%
%Simulations have been done using the PILCO toolbox 
%In this work a radial basis function (RBF) controller is used.
%+Results+
\section{MODEL-FREE POLICY SEARCH}

\begin{figure*}[!thp] 
	\centering
	\begin{subfigure}{0.325\textwidth}
	\includegraphics[width=\textwidth]{julia_sdm_quad.png}
	\caption{Mass-Spring-Damper \newline}\label{fig:PoWER_sdm}
	\end{subfigure}
	\begin{subfigure}{0.325\textwidth}
	\includegraphics[width=\textwidth]{julia_pend_quad.png}
	\caption{Pendulum \newline}\label{fig:PoWER_pendulum}
	\end{subfigure}
	\begin{subfigure}{0.325\textwidth}
	\includegraphics[width=\textwidth]{julia_cp10_quad.png}
	\caption{Cartpole, initial angle: 10\textdegree}\label{fig:PoWER_cp}
	\end{subfigure}\caption{Figures (a)-(c) show the system output over time as well as the reward at the increasing iteration steps.}\label{fig:PoWER}
\end{figure*} 
This section shows an approach of a model-free policy search using Policy learning by Weighting Exploration with Returns (PoWER).
\subsection{Algorithm}
The PoWER-algorithm is based on applying varying policies to the unknown system and optimizing these policies according to their rewards.
The policy $\pi_\theta$ applied on the system in this work is based on Dynamic Movement Primitives (DMP)
\begin{equation}
\pi_{\theta}=\ddot{y}=\tau^{2}\alpha_y\beta_y(g-y(t))-\tau^{2}\alpha_y \dot{y}(t)+\tau^{2}f(z(t))
\end{equation}
with parameters $\tau,\alpha_y, \beta_y>0$ \cite{DMP_web}.
The function $f$ is represented by a radial basis function (RBF) network 
\begin{equation}
f(z(t))=\frac{\sum_{i=1}^N \phi_i \omega_i}{\sum_{i=1}^N \phi_i}z 
\end{equation}
where $\omega_i$ corresponds to the weights of the RBFs $\phi_i$. Usually five to twenty radial basis functions $\phi_i$ per system's degree of freedom (DoF) are used. The variable $z(t)$ is a system state resulting from the dynamics 
\begin{equation}
\dot{z}=-\tau\,\alpha_z \,z 
\end{equation}
with $\alpha_z>0$ \cite{DMP_web}.
To optimize the policy $\pi_\theta$, the policy parameter vector $\theta$ (containing weights $\omega_i$ of RBF $\phi_i$) are explored in parameter space with adding a gaussian noise $\varepsilon \sim N(0,\sigma_{\varepsilon})$ to the parameters in iteration $n$ \cite{kober2009policy}
\begin{equation}
\theta_{temp}=\theta_n+\varepsilon
\end{equation}
The policy obtained by temporary parameters $\theta_{temp}$ are rolled out on the system and the system trajectory $x_t$ is measured. A cumulative reward $R$ assesses the quality of the rollout using the trajectory $x_t$: %for different $\theta_{temp}$: 
%$x_t$ represent the system's target states and $r_t$ is based on a squared exponential function.
%$r_t=e^{-\alpha_1(x_1-x{t,1})^{2}-...}$
\begin{equation}
R(\theta_{temp})=\sum_{t=0}^{T-1} r_t(x_t) 
\end{equation}
The reward $r_t$ at time $t$ is calculated using a squared exponential function. The procedure of applying different policies using different parameters $\theta_{temp}$, which are obtained by the gaussian noise added to the base parameter $\theta_n$, is repeated $k$ times at each iteration $n$. The parameter update for the next iteration $n+1$ is then done by ordering the temporary parameters $\theta_{temp}$ by their rewards $R(\theta_{temp})$ from the best to the worst. The update law is then
\begin{equation}
\theta_{n+1}=\theta_n+\frac{\Delta\theta_1 R(\theta_1)+...+\Delta\theta_{10} R(\theta_{10})}{R(\theta_1)+...+R(\theta_{10})}
\end{equation}
with $\Delta\theta_k=\theta_{temp,k}-\theta_n$, $k=1,2,...,10$. 
%In other words only the best ten rollouts are taken into account:
The mentioned steps are repeated until the reward $R(\theta_n)$ is greater than a threshold close to 1 or a maximum number of iterations is reached.


%used for the different rollouts 
%Having initialized the weighting parameters $\theta_{temp},\theta_n=0$,
%a white gaussian noise is added in the beginning of the rollout: 
%$\theta_{temp}=\theta_n+\varepsilon$ with $\varepsilon \sim N(0,\sigma_{\varepsilon})$. $\theta_{temp}$ is used to weight the RBFs $\phi_i(z)=e^{-\frac{1}{2\sigma_i^{2}}(z-\mu_i)^{2}}$ while training period. While $\mu$ indicates the expected value and thus the position of the distribution's maximum, $\sigma$ is the standard deviation.     

%$=\theta_{temp}$ and  ($\alpha_z>0$), $f(z(t))$ has to be optimized.
%
%Applying the policy on the system, the result is the time behaviour of the rollout. 

\subsection{Results}
In figure \ref{fig:PoWER} 
%the following%
the results of the three systems mass-spring-damper, pendulum and cartpole are shown. For each system it takes around 3.000 trials to reach the target positions with a sufficient quality (see figure \ref{fig:PoWER}). At first the three reward curves of the systems show a fast improvement. After about 500 trials the rewards of the mass-spring-damper and the pendulum show a moderated linear improvement; the reward of the cartpole has its significant point at about 1.500 trials. The appearance of the three graphs differ: While the reward of the mass-spring-damper system shows comparatively little scattering and a maximum of about 0.75 (out of 1), the rewards of the two other systems have larger variations and lower maxima. This is due to the fact that leaving a point with higher rewards, entails a higher penalty in these two systems. Because the policy decreases with $z$, it is difficult for the alorithm to hold the target position in the end of the rollout horizon.


\section{OPTIMAL CONTROL}
\begin{figure*}[!thp] 
	\centering
	\begin{subfigure}{0.325\textwidth}
		\includegraphics[width=\textwidth]{MS.png}
		\caption{Comparison of LQR and LQR+PI. Position $x_1$ of the spring-mass system (top) and control input $u$ (bottom).}\label{fig:JanSM}
	\end{subfigure}
	\begin{subfigure}{0.325\textwidth}
		\includegraphics[width=\textwidth]{P1.png}
		\caption{Comparison of iLQR result and closed-loop simulation. Angular position $x_1$ of the pendulum (top) and control input $u$ (bottom).}\label{fig:JanP1}
	\end{subfigure}
	\begin{subfigure}{0.325\textwidth}
		\includegraphics[width=\textwidth]{CP.png}
		\caption{MPC of the cart-pole. Angle $\theta=x_3$ of the pendulum and position $x=x_1$ of the car (top) and control input $u$ (bottom).}\label{fig:JanCP}
	\end{subfigure}\caption{Plots (a) - (b) show the state trajectories for the three systems. Mass-spring (a), Pendulum (b), Cart pole (c).}\label{fig:PILCO}
\end{figure*} 
The overarching concept in optimal control is to optimize control inputs and system states according to a specific cost function. 

%For linear systems, the method of choice is a linear-quadratic regulator (LQR) that optimizes a linear-quadratic (LQ) cost function. When dealing with non-linear systems, LQR can be iteratively applied to locally LQ problems. This method is called iterative LQR (iLQR). Another method to handle non-linear systems is model-predictive control (MPC). In MPC, an optimization problem to obtain optimal inputs and state trajectories is only solved for a finite time horizon and continually updated during runtime.

Since there are many different forms of optimal control, in this section, LQR was applied to the linear spring-mass system, iLQR to the pendulum, and MPC to the cart-pole system. This highlights the different capabilities of these approaches.
\subsection{Algorithms}


The continuous LQR aims at minimizing the cost function depicted in (\ref{eqn:JanSMcostfunc}).
\begin{equation}\label{eqn:JanSMcostfunc}
	J_1 = \int^{\infty}_{0} \left(x^{T}Q\,x + u^{T}R\,u\right)dt,
\end{equation}
with weight matrices $Q$ and $R$.
Since there was no specifications for input and states, the weights were simply set to $Q=$ diag$(1,1)$ and $R=0.1$. 
This results in the well known control law $u=-K\,x$ which transforms (1) into
\begin{equation}\label{eqn:JanSMcontrolledSys}
	\dot{x} = (A-B\,K)x.
\end{equation}
$K$ is the optimal state feedback gain.

With a goal position different from the origin, a prefilter matrix $L$ as shown in (\ref{eqn:JanSMprefilter}) is necessary.
\begin{equation}\label{eqn:JanSMprefilter}
L = \left(C(B\,K-A)^{-1}B\right)^{-1}
\end{equation}
Since LQR is lacking an integral component, a simple PI controller ($K_P=K_I=1$) was added to drive the system into the desired goal position even under disturbances. Note that the system would also be stable for any other controller gain but experience higher overshooting.\\

The iLQR approach for the pendulum was based on \cite{Opti_iLQR} and their Matlab toolbox. 

%iLQR (or the similar iLQG) algorithm is a shooting method. An input trajectory is applied to the system (forward pass), and subsequently optimized (backwards pass). This process is repeated iteratively, until the trajectory converges.

The input was limited to $|u|<4$\,Nm, so that the goal can only be reached by swinging back and forth. 

The cost function for the pendulum problem is defined as 
\begin{equation}\label{eqn:JanPcostfunc}
J_2 = \frac{1}{2}\overline{x}_N^{T}P\,\overline{x}_N + \frac{1}{2}\sum_{k=0}^{N-1}\left(\overline{x}_k^{T}Q\,\overline{x}_k + u_k^{T}R\,u_k\right),
\end{equation}
with the time horizon $N$, the deviation of the current state from the desired state $\overline{x}_k=(x_k-x_{des})$, the final weight matrix $P$, and the familiar $Q$ and $R$ matrices. 

Without any other specifications, the input weight was set to a rather low value of $R=10^{-3}$ and $Q=$ diag$(10^{-3},0)$ to allow for necessary swinging and full speed during the run. The final cost was set to $P=$ diag$(100,0.1)$ to ensure the upright position.

The iLQR algorithm was set to a time horizon of $N=5000$ at a $t=1$\,ms sample time, which equals a runtime of 5 seconds.

iLQR is an open-loop controller. Therefore, in the implementation, the resulting input trajectory $\underline{u}$ from the optimization was used as feed forward control, while a PID controller minimized the difference between the estimated trajectory $\underline{x}$ (from the optimization) and the actual state.
Without this setup, even the slightest perturbation of the system (e.g. rounding errors) leads to a failed upswing since the goal position is an unstable equilibrium point.\\

The MPC implementation was based on \cite{Opti_MPC}. Here, the Matlab function ``fmincon'' is used to solve the optimization.

The input was constrained to $|u|<5$\,N, and the cart position was limited to $|x_1|<4$\,m. The cost function is the same as in (\ref{eqn:JanPcostfunc}), with $P=$ diag$(0,10,10,10)$ (end position of the cart is irrelevant as long as it stays within the bounds), $Q=$ diag$(20,10,10,0)$ (cart should remain in the center during upswing, while angular speed is irrelevant), and $R=0.01$. 

The prediction horizon was set to $N=20$ at a sample time of $t=20$\,ms, and the simulation ran for 4 seconds.

Since MPC is a closed-loop control strategy, no additional controller was necessary.
\subsection{Results}


For the spring-mass system without disturbances, the goal was reach by the LQR controller. However, with an added constant disturbance of $d=0.5$\,N, the PI controller was necessary as shown in figure (\ref{fig:JanSM}). Note that using the PI controller instead of LQR would cause the system to become unstable.\\
%\begin{figure}[htp] 
%	\centering
%	\includegraphics[width=0.5\textwidth]{MS.png}
%	\caption{Comparison of LQR and LQR+PI controller. Position $x_1$ of the spring-mass system (top) and control input $u$ (bottom).}
%	\label{fig:JanSM}
%\end{figure} 

For the pendulum, the iLQR algorithm converged after 52 iterations. The resulting trajectory is one initial swing to the left, followed by a complete upswing to the right. Figure (\ref{fig:JanP1}) displays the stand-alone iLQR trajectory and one with an added PID controller.
%\begin{figure}[htp] 
%	\centering
%	\includegraphics[width=0.5\textwidth]{P1.png}
%	\caption{Comparison of iLQR result and closed-loop simulation. Angular position $x_1$ of the pendulum (top) and control input $u$ (bottom).}
%	\label{fig:JanP1}
%\end{figure} 

Both trajectories are almost identical, and there is only a small deviation in the control input at the end of the upswing (compare figure (\ref{fig:JanP1}) bottom at 3 seconds). However, this slight change is crucial to keep the pendulum in the upright position. Without the added PID controller, the system remains instable as shown in figure (\ref{fig:JanP2}), since iLQR is not a closed-loop controller.\\
%\begin{figure}[htp] 
%	\centering
%	\includegraphics[width=0.5\textwidth]{P2.png}
%	\caption{Comparison of iLQR result and open-loop simulation. Angular position $x_1$ of the pendulum.}
%	\label{fig:JanP2}
%\end{figure}\\

The MPC controller managed to stabilize the pendulum of the cart-pole in the upright position for all initial angles. The successful swing-up for an initial position of 10 is presented in figure (\ref{fig:JanCP}). 


%\begin{figure}[htp] 
%	\centering
%	\includegraphics[width=0.5\textwidth]{CP.png}
%	\caption{The swing-up of the cart-pole system. Angular position $\theta=x_3$ of the pendulum and horizontal displacement $x=x_1$ of the car (top) and control input $u$ (bottom).}
%	\label{fig:JanCP}
%\end{figure}

\section{COMPARISON AND DISCUSSION}
As the results show, optimal control is an effective method as it does not need any interactions with the real system and still delivers high accuracy with reasonable computational effort. However, the system model has to be known or derived by linearization, and the parameterization of the weights requires some work.
For unknown or highly non-linear models this method might fail, which is when the presented reinforcment learning approaches appear to be valid options. The advantage of PILCO is that model learning and simulative parameter optimizations require only few system interactions for stable closed loop control. The optimization algorithm is, however, computationally expensive. 
By contrast, the calculation of DMP-policies  is faster, but requires several system interactions to yield an (open loop) policy. Both RL algorithms show slight deviations around the goal position due to their inherent structure (see figures. 1 (c); 2 (b), (c)).

%By comparing the results shown above -> If model is known or linearization is a good approximation of the model , optimal control works very effective regarding system interactions, accuracy, computation and time effort. However, parametrization can be XXX. % compared to PILCO and MF-RL.Parametrisierung etwas aufwendig. 
%Otherwise reinforcement learning approaches like PILCO or DMP-policies are a valid option. The advantage of PILCO is that model learning and simulative parameter optimizations only require few system interactions for stable closed loop control. However, the algorithm is computationally expensive. Contrarily, the optimization of DMP-policies  is faster, but requires many system interactions to yield an open loop policy. However, the RL algorithms show slight inacurracy due to random exploration and/or system learning (fig. 1c 2a und c).

\section{CONCLUSION}
Summarizing the results, optimal control presents a good starting point for controlling a system when the dynamics are (partially) known. More complex sub-dynamics (e.g. friction) can then be tackled with reinforcement learning. Here, DMP policies appear to be a good choice for repetitive tasks, while PILCO is a strong algorithm for a general control scheme.



\begin{thebibliography}{99}

\bibitem{PILCO_paper} Deisenroth, Marc Peter. Efficient reinforcement learning using Gaussian processes. Vol. 9. KIT Scientific Publishing, 2010.
\bibitem{GP_deisenroth} Deisenroth, Marc Peter, Dieter Fox, and Carl Edward Rasmussen. "Gaussian processes for data-efficient learning in robotics and control." IEEE transactions on pattern analysis and machine intelligence 37.2 (2015): 408-423.
\bibitem{PILCO_web} Deisenroth, Marc Peter, Dieter Fox, and Carl Edward Rasmussen. "PILCO website", 2013. [Online]. Available: http://mlg.eng.cam.ac.uk/pilco/. [Accessed: 27- Jan- 2019]
\bibitem{DMP_web} Studywolf. "Dynamic Movement Primitives Part 1: The Basics", 2013. [Online]. Available: https://studywolf.wordpress.com/2013/11/16/dynamic-movement-primitives-part-1-the-basics/. [Accessed: 28-Jan- 2019]
\bibitem{kober2009policy} Jens Kober, Jan Peters. "Policy search for motor primitives in robotics". Advances in neural information processing systems (2009): 849-856.
\bibitem{Opti_iLQR} Tassa, Mansard, and Todorov. "Control-limited differential dynamic programming". IEEE International Conference on Robotics and Automation (ICRA). 2014.
\bibitem{Opti_MPC} Gruene, and Pannek. "Nonlinear Model Predictive Control". Springer Verlag. 2017.

%\bibitem{c1} G. O. Young, ÒSynthetic structure of industrial plastics (Book style with paper title and editor),Ó 	in Plastics, 2nd ed. vol. 3, J. Peters, Ed.  New York: McGraw-Hill, 1964, pp. 15Ð64.
%\bibitem{c2} W.-K. Chen, Linear Networks and Systems (Book style).	Belmont, CA: Wadsworth, 1993, pp. 123Ð135.
%\bibitem{c3} H. Poor, An Introduction to Signal Detection and Estimation.   New York: Springer-Verlag, 1985, ch. 4.
%\bibitem{c4} B. Smith, ÒAn approach to graphs of linear forms (Unpublished work style),Ó unpublished.
%\bibitem{c5} E. H. Miller, ÒA note on reflector arrays (Periodical styleÑAccepted for publication),Ó IEEE Trans. Antennas Propagat., to be publised.
%\bibitem{c6} J. Wang, ÒFundamentals of erbium-doped fiber amplifiers arrays (Periodical styleÑSubmitted for publication),Ó IEEE J. Quantum Electron., submitted for publication.
%\bibitem{c7} C. J. Kaufman, Rocky Mountain Research Lab., Boulder, CO, private communication, May 1995.
%\bibitem{c8} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ÒElectron spectroscopy studies on magneto-optical media and plastic substrate interfaces(Translation Journals style),Ó IEEE Transl. J. Magn.Jpn., vol. 2, Aug. 1987, pp. 740Ð741 [Dig. 9th Annu. Conf. Magnetics Japan, 1982, p. 301].
%\bibitem{c9} M. Young, The Techincal Writers Handbook.  Mill Valley, CA: University Science, 1989.
%\bibitem{c10} J. U. Duncombe, ÒInfrared navigationÑPart I: An assessment of feasibility (Periodical style),Ó IEEE Trans. Electron Devices, vol. ED-11, pp. 34Ð39, Jan. 1959.
%\bibitem{c11} S. Chen, B. Mulgrew, and P. M. Grant, ÒA clustering technique for digital communications channel equalization using radial basis function networks,Ó IEEE Trans. Neural Networks, vol. 4, pp. 570Ð578, July 1993.
%\bibitem{c12} R. W. Lucky, ÒAutomatic equalization for digital communication,Ó Bell Syst. Tech. J., vol. 44, no. 4, pp. 547Ð588, Apr. 1965.
%\bibitem{c13} S. P. Bingulac, ÒOn the compatibility of adaptive controllers (Published Conference Proceedings style),Ó in Proc. 4th Annu. Allerton Conf. Circuits and Systems Theory, New York, 1994, pp. 8Ð16.
%\bibitem{c14} G. R. Faulhaber, ÒDesign of service systems with priority reservation,Ó in Conf. Rec. 1995 IEEE Int. Conf. Communications, pp. 3Ð8.
%\bibitem{c15} W. D. Doyle, ÒMagnetization reversal in films with biaxial anisotropy,Ó in 1987 Proc. INTERMAG Conf., pp. 2.2-1Ð2.2-6.
%\bibitem{c16} G. W. Juette and L. E. Zeffanella, ÒRadio noise currents n short sections on bundle conductors (Presented Conference Paper style),Ó presented at the IEEE Summer power Meeting, Dallas, TX, June 22Ð27, 1990, Paper 90 SM 690-0 PWRS.
%\bibitem{c17} J. G. Kreifeldt, ÒAn analysis of surface-detected EMG as an amplitude-modulated noise,Ó presented at the 1989 Int. Conf. Medicine and Biological Engineering, Chicago, IL.
%\bibitem{c18} J. Williams, ÒNarrow-band analyzer (Thesis or Dissertation style),Ó Ph.D. dissertation, Dept. Elect. Eng., Harvard Univ., Cambridge, MA, 1993. 
%\bibitem{c19} N. Kawasaki, ÒParametric study of thermal and chemical nonequilibrium nozzle flow,Ó M.S. thesis, Dept. Electron. Eng., Osaka Univ., Osaka, Japan, 1993.
%\bibitem{c20} J. P. Wilkinson, ÒNonlinear resonant circuit devices (Patent style),Ó U.S. Patent 3 624 12, July 16, 1990. 






\end{thebibliography}




\end{document}